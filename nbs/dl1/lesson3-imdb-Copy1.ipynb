{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's download the dataset we are going to study. The [dataset](http://ai.stanford.edu/~amaas/data/sentiment/) has been curated by Andrew Maas et al. and contains a total of 100,000 reviews on IMDB. 25,000 of them are labelled between positive and negative for training, another 25,000 are labelled for testing (in both cases they are highly polarized). The remaning 50,000 is an additional unlabelled data (but we will find a use for it nonetheless).\n",
    "\n",
    "We'll begin with a sample we've prepared for you, so that things run quickly before going over the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jupyter/.fastai/data/imdb_sample/texts.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://files.fast.ai/data/examples/imdb_sample'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URLs.IMDB_SAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only contains one csv file, let's have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  is_valid\n",
       "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
       "1  positive  This is a extremely well-made film. The acting...     False\n",
       "2  negative  Every once in a long while a movie will come a...     False\n",
       "3  positive  Name just says it all. I watched this movie wi...     False\n",
       "4  negative  This movie succeeds at being one of the most u...     False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path/'texts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains one line per review, with the label ('negative' or 'positive'), the text and a flag to determine if it should be part of the validation set or the training set. If we ignore this flag, we can create a DataBunch containing this data in one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextDataBunch.from_csv(path, 'texts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing this line a process was launched that took a bit of time. Let's dig a bit into it. Images could be fed (almost) directly into a model because they're just a big array of pixel values that are floats between 0 and 1. A text is composed of words, and we can't apply mathematical functions to them directly. We first have to convert them to numbers. This is done in two differents steps: tokenization and numericalization. A `TextDataBunch` does all of that behind the scenes for you.\n",
    "\n",
    "Before we delve into the explanations, let's take the time to save the things that were calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time we launch this notebook, we can skip the cell above that took a bit of time (and that will take a lot more when you get to the full dataset) and load those results like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextDataBunch.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of processing we make texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:\n",
    "\n",
    "- we need to take care of punctuation\n",
    "- some words are contractions of two different words, like isn't or don't\n",
    "- we may need to clean some parts of our texts, if there's HTML code for instance\n",
    "\n",
    "To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <col width='90%'>  <col width='10%'>  <tr>\n",
       "    <th>text</th>\n",
       "    <th>label</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 raising victor vargas : a review \\n\\n you know , raising victor vargas is like sticking your hands into a big , xxunk bowl of xxunk . it 's warm and gooey , but you 're not sure if it feels right . try as i might , no matter how warm and gooey raising victor vargas became i was always aware that something did n't quite feel right . victor vargas suffers from a certain xxunk on the director 's part . apparently , the director thought that the ethnic backdrop of a latino family on the</th>\n",
       "    <th>negative</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 now that che(2008 ) has finished its relatively short australian cinema run ( extremely limited xxunk screen in xxunk , after xxunk ) , i can xxunk join both xxunk of \" at the movies \" in taking steven soderbergh to task . \\n\\n it 's usually satisfying to watch a film director change his style / subject , but soderbergh 's most recent stinker , the girlfriend xxunk ) , was also missing a story , so narrative ( and editing ? ) seem to suddenly be soderbergh 's main challenge . strange , after xxunk years</th>\n",
       "    <th>negative</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 many xxunk that this is n't just a classic due to the fact that it 's the first 3d game , or even the first xxunk - up . it 's also one of the first xxunk games , one of the xxunk definitely the first ) truly claustrophobic games , and just a pretty well - xxunk gaming experience in general . with graphics that are terribly dated today , the game xxunk you into the role of xxunk even * think * i 'm going to attempt spelling his last name ! ) , an american</th>\n",
       "    <th>positive</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 i really wanted to love this show . i truly , honestly did . \\n\\n for the first time , gay viewers get their own version of the \" the bachelor \" . with the help of his obligatory \" hag \" xxunk , james , a good looking , well - to - do thirty - something has the chance of love with 15 suitors ( or \" mates \" as they are referred to in the show ) . the only problem is half of them are straight and james does n't know this . if</th>\n",
       "    <th>negative</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 this film sat on my xxunk for weeks before i watched it . i xxunk a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the xxunk - up xxunk of new xxunk . \\n\\n the format is the same as max xxunk ' \" la xxunk , \" based on a play by arthur xxunk , who is given an \" inspired by \" credit . it starts from one person , a prostitute , standing on a street xxunk in brooklyn . she is picked</th>\n",
       "    <th>positive</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 how viewers react to this new \" adaption \" of shirley jackson 's book , which was promoted as xxup not being a remake of the original 1963 movie ( true enough ) , will be based , i suspect , on the following : those who were big fans of either the book or original movie are not going to think much of this one ... and those who have never been exposed to either , and who are big fans of hollywood 's current trend towards \" special effects \" being the first and last word</th>\n",
       "    <th>negative</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 to review this movie , i without any doubt would have to quote that memorable scene in tarantino 's \" pulp fiction \" ( xxunk ) when jules and vincent are talking about mia wallace and what she does for a living . jules tells vincent that the \" only thing she did worthwhile was pilot \" . vincent asks \" what the hell is a pilot ? \" and jules goes into a very well description of what a tv pilot is : \" well , the way they make shows is , they make one show</th>\n",
       "    <th>negative</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 \\n\\n i 'm sure things did n't exactly go the same way in the real life of homer hickam as they did in the film adaptation of his book , rocket boys , but the movie \" october sky \" ( an xxunk of the book 's title ) is good enough to stand alone . i have not read hickam 's memoirs , but i am still able to enjoy and understand their film adaptation . the film , directed by joe xxunk and written by lewis xxunk , xxunk the story of teenager homer hickam (</th>\n",
       "    <th>positive</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 well , what can i say . \\n\\n \" what the xxunk do we know \" has achieved the nearly impossible - leaving behind such masterpieces of the genre as \" the xxunk \" , \" the xxunk master \" , \" xxunk \" , and so fourth , it will go down in history as the single worst movie i have ever seen in its xxunk . and that , ladies and gentlemen , is impressive indeed , for i have seen many a bad movie . \\n\\n this masterpiece of modern cinema consists of two xxunk</th>\n",
       "    <th>negative</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 the trouble with the book , \" memoirs of a geisha \" is that it had japanese xxunk but underneath the xxunk it was all an american man 's way of thinking . reading the book is like watching a magnificent ballet with great music , sets , and costumes yet performed by xxunk animals dressed in those xxunk far from japanese ways of thinking were the characters . \\n\\n the movie is n't about japan or real geisha . it is a story about a few american men 's mistaken ideas about japan and geisha xxunk through</th>\n",
       "    <th>negative</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = TextClasDataBunch.load(path)\n",
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: \n",
    "- the \"'s\" are grouped together in one token\n",
    "- the contractions are separated like his: \"did\", \"n't\"\n",
    "- content has been cleaned for any HTML symbol and lower cased\n",
    "- there are several special tokens (all those that begin by xx), to replace unkown tokens (see below) or to introduce different text fields (here we only have one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at list twice with a maximum vocabular size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.\n",
    "\n",
    "The correspondance from ids tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we look at what a what's in our datasets, we'll see only numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 40, 34, 170, 61, 6, 350, 3, 46, 1492]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds[0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds[2][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the data block API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the data block API with NLP and have a lot more flexibility thant what the default factory methods offer. In the previous example for instance, the data was randomly split between train and validation instead of reading the third column of the csv.\n",
    "\n",
    "With the data block API though, we have to manually call the tokenize and numericalize steps. This allows more flexibility, and if you're not using the defaults from fastai, the variaous arguments to pass will appear in the step they're revelant, so it'll be more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (TextSplitData.from_csv(path, 'texts.csv', input_cols=1, label_cols=0, valid_col=2)\n",
    "        .datasets(TextDataset)\n",
    "        .tokenize() #can specify custom arguments for tokenization here\n",
    "        .numericalize() #can specify custom arguments for numericalization here\n",
    "        .databunch(TextDataBunch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab the full dataset for what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jupyter/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/README'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/test'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/train')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jupyter/.fastai/data/imdb/train/labeledBow.feat'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/train/pos'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/train/unsupBow.feat'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/train/neg'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/train/unsup')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews are in a training and test set following an imagenet structure. The only difference is that there is an `unsup` folder in `train` that contains the unlabelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipeia called [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset)). That model has been trained to guess what the next word, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n",
    "\n",
    "We are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviex lefts by people on IMDB isn't the same as the English of wikipedia, we'll need to adjust a little bit the parameters of our model. Plus there might be some words extremely common in that dataset that were barely present in wikipedia, and therefore might no be part of the vocabulary the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = (TextFileList.from_folder(path)         \n",
    "           #grab all the text files in path\n",
    "           .label_const(0)           \n",
    "           #label them all wiht 0s (the targets aren't positive vs negative review here)\n",
    "           .split_by_folder(valid='test')         \n",
    "           #split by folder between train and validation set\n",
    "           .datasets() \n",
    "           #use `TextDataset`, the flag `is_fnames=True` indicates to read the content of the files passed\n",
    "           .tokenize()\n",
    "           #tokenize with defaults from fastai\n",
    "           .numericalize()\n",
    "           #numericalize with defaults from fastai\n",
    "           .databunch(TextLMDataBunch))\n",
    "           #use a TextLMDataBunch\n",
    "\n",
    "data_lm.save('tmp_lm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.\n",
    "\n",
    "The line before being a bit long, we want to load quickly the final ids by using the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here if re-running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/jupyter/.fastai/data/imdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter/.fastai/data/imdb')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <col width='5%'>  <col width='95%'>  <tr>\n",
       "    <th>idx</th>\n",
       "    <th>text</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>xxfld 1 avenging eagle is the story of a man chi ming - sing ( ti lung ) who after years of killing for a brutal gang decides to leave and seek revenge against them and their leader . the gang , know as the iron boat clan and in particular the men he raided with the 13 eagles . all were raised from birth by yoh xi - hung ( ku feng ) to be brutal killers to obey his every command . each with his own hand made eagle medallion and weapon .</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>to be there ) . the son sees them together laughing and jumps to conclusions . he promptly leaves not telling his father what he 's seen . but , nothing ever happened ! but the son , played by an actor i do n't know , goes ballistic . \\n\\n the only problem i have is that the actor seems to get a little over - the - top . but , then again it is his strong reaction which is the catapult of all that happens later in the film . \\n\\n what</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>school in edinburgh . apparently they did n't bother waiting until the people died in order to make a few pounds -- often suffocating their hapless victims ! ! \\n\\n years later , in 1960 , another variation of the william burke and william hare story came to the theaters , but this one ( starring peter cushing ) was not based on the stevenson novel but the actual crimes . while the lewton movie was marvelous and horrifying , this one seems worth seeing as well because it is more realistic and unflinching (</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>you wo n't be able to redeem your one - and - a - half hours by just listening to the soundtrack . it 's crap too . all of the dramatic tension is relieved after forty minutes , which means that the film wanders aimlessly and pointlessly for another forty . omar sharif 's commentary on the xxup dvd is similarly inane . there is really nothing to be said in favour of this movie along any dimension . xxfld 1 i do n't want to write a comment for this useless and completely</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>have been great . in the middle of a dramatic film , these shots just feel like padding with bad acting . \\n\\n the story itself , although relatively simple -- too simple , perhaps , is chopped up and told as if it 's going to have some big revelation or twist . altman keeps unnecessarily jumping back and forth in time -- but just a few hours , and he keeps unnecessarily jumping back and forth between different sets of characters in the middle of ( very ) long scenes . i guess</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>, thinner man ( the stunt guy is actually wearing fake fat to look like steven ) , who has a lot more physical talent than steven had in his prime . this is further compounded by the fact that the stunt man performs stunts that steven could never perform ( jump kicks , jump spin kick , and all other stunts involving jumping high ) . \\n\\n the movie seems to have been filmed entirely in bangkok , thailand and recruited what seems to be the entire cast of xxunk bak , minus tony</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>can & will do much better . \\n\\n i was n't expecting a block buster here but come on some crappy low budget film could at least try harder . the effects were so overly cheap its not funny . xxfld 1 years have past since alex rain ( played by olivier gruner in the first movie ) stumbled onto the horrific plot that involved replacing humans with machines however since then a war between cyborgs and humans has emerged and we lost , now a superwoman of sorts who is the daughter of olivier</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>for and about her ! i have a hard time understanding xxup why she agreed to do this film because it hits so close to home . in other words , the title character is pretty much the real joan crawford . both were extreme xxunk , very controlling , emotionally constricted and could not ( or claimed they could not in the case of harriet craig ) have kids . and , of course , both had serious problems having a normal intimate relationship with those around them . while the biographies i have</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>and laura . all said and done , this is a lousy picture and i purchased the xxup dvd for only $ 1.50 and i really got ripped xxup off ! xxfld 1 bad plot , bad dialogue , bad acting , idiotic directing , the annoying porn groove soundtrack that ran continually over the overacted script , and a crappy copy of the xxup vhs can not be redeemed by consuming liquor . trust me , because i stuck this turkey out to the end . it was so pathetically bad all over that</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>got that way is what people want to know . \\n\\n as far as this movie xxrep 4 . the book was good , even if it was a little derivative of other stories from the \" be careful what you wish for \" genre . burke plays an overweight lawyer who kills the daughter of a gypsy and is cursed by her father ( constantine from tv 's \" room 222 \" ) to several pounds a day . \\n\\n like i said , it starts out good , but why involve the mobster</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = TextLMDataBunch.load(path, 'tmp_lm')\n",
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in './fastai/models/' (or elsewhere if you specified different paths in your config file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bptt default is 75 but ran out of memory using default\n",
    "learn = language_model_learner(data_lm, drop_mult=0.3, pretrained_model=URLs.WT103, bptt=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 18:05\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      4.593635    4.445621    0.250231  (18:05)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=<fastai.text.data.TextLMDataBunch object at 0x7f4828445d68>, model=SequentialRNN(\n",
       "  (0): RNNCore(\n",
       "    (encoder): Embedding(60002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=60002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function cross_entropy at 0x7f47b93cec80>, metrics=[<function accuracy at 0x7f47b2972c80>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/jupyter/.fastai/data/imdb'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[RNNTrainer(learn=LanguageLearner(data=<fastai.text.data.TextLMDataBunch object at 0x7f4828445d68>, model=SequentialRNN(\n",
       "  (0): RNNCore(\n",
       "    (encoder): Embedding(60002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=60002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function cross_entropy at 0x7f47b93cec80>, metrics=[<function accuracy at 0x7f47b2972c80>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/jupyter/.fastai/data/imdb'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Embedding(60002, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60002, 400, padding_idx=1)\n",
       "  )\n",
       "  (2): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=60002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")]), bptt=60, alpha=2.0, beta=1.0, adjust=False)], layer_groups=[Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Embedding(60002, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60002, 400, padding_idx=1)\n",
       "  )\n",
       "  (2): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=60002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('fit_head')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the fine-tuning, we can then unfeeze and launch a new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 19:43\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      4.279771    4.219566    0.275002  (19:43)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fine_tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 19:41\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      4.196813    4.166374    0.281702  (19:41)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:00\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my own'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict('my')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fine_tuned-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 19:39\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      4.165388    4.134218    0.285746  (19:39)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to save the model but also it's encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextFileList.from_folder(path)\n",
    "             #grap all the text files in path\n",
    "            .label_from_folder(classes=['neg','pos'])\n",
    "             #label them all with their folder, only keep 'neg' and 'pos'\n",
    "            .split_by_folder(valid='test')\n",
    "             #split by folder between train and validation set\n",
    "            .datasets()\n",
    "             #use `TextDataset`, the flag `is_fnames=True` indicates to read the content of the files passed\n",
    "            .tokenize()\n",
    "             #tokenize with defaults from fastai\n",
    "            .numericalize(vocab = data_lm.vocab)\n",
    "             #numericalize with the same vocabulary as our pretrained model\n",
    "            .databunch(TextClasDataBunch, bs=50))\n",
    "data_clas.save('tmp_clas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <col width='90%'>  <col width='10%'>  <tr>\n",
       "    <th>text</th>\n",
       "    <th>label</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 match 1 : tag team table match bubba ray and spike dudley vs eddie guerrero and chris benoit bubba ray and spike dudley started things off with a tag team table match against eddie guerrero and chris benoit . according to the rules of the match , both opponents have to go through tables in order to get the win . benoit and guerrero heated up early on by taking turns hammering first spike and then bubba ray . a german suplex by benoit to bubba took the wind out of the dudley brother . spike tried to</th>\n",
       "    <th>pos</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 i wo n't try to speculate as to what brando was attempting . at his best he turns in such oddball performances , insinuating so many things at once , that it does n't seem he does anything so much as play by unfailing instinct . often it seems he is calling attention to some favored aspect of his character over all others , a concentration which , if followed , turns out something of a red herring , as he turns out subtler , craftier than at first appeared . this is a mastery of artifice ,</th>\n",
       "    <th>pos</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 this was the second of two filmed \" hamlets \" in the nineties , the first being franco zeffirelli 's , starring mel gibson , from 1990 . zeffirelli 's version , like laurence olivier 's from 1948 , was based upon an abridged version of the play , with much of shakespeare 's original text being cut . ( i have never seen tony richardson 's 1969 version , but as that ran to less than two hours , shorter even than zeffirelli 's , i presume that was also abridged ) . kenneth branagh was attempting</th>\n",
       "    <th>pos</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 it 's always difficult to put a stamp on any film as being ' the best , ' whether of all time , a certain genre , or what have you , but i believe a strong argument could be made that in fact , laputa is the greatest animated film ever made . it is in my mind the masterwork of hayao miyazaki , the most talented of japan 's animated directors , and it best captures his strengths as a director , storyteller , and designer , as well as encapsulating all of his favorite underlying</th>\n",
       "    <th>pos</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 when i first popped in happy birthday to me , i checked the timer to see how long the film was . i was amazed at the length . both animated and horror films share a common ground : attention span of the selected audience and that should be at or right around 90 minutes . anything more , and you 'll lose the bulk of your audience . \\n\\n this 110 minutes , or 20 minutes past its prime was a huge problem for me . i 'd like to say half of this movie could 've</th>\n",
       "    <th>neg</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 linking story : another first - time viewing for me and , again , this is one of the most popular of the amicus anthologies - and it 's easy to see why , though i realize how the film 's rather meaningless title could be misleading for some ; i certainly fancied director peter duffell 's choice - xxup death xxup and xxup the xxup maiden ( which , incidentally , is a classical piece by schubert that is heard in the film during the peter cushing episode ) - a great deal more . though the</th>\n",
       "    <th>pos</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 this comment does contain spoilers ! ! \\n\\n there are few actors that have an intangible to them . that innate quality which is an amalgamation of charisma , panache and swagger . it 's the quality that can separate good actors from the truly great . i think george clooney has it and so does jack nicholson . you can look at clooney 's subtle touches in scenes like his one word good - bye to andy garcia in ocean 's 11 when they just utter each other 's name xxunk . \" terry . \" \"</th>\n",
       "    <th>pos</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 was this a comedy or was it a drama ? i begin this review by asking this question because the film that i just witnessed , hollywood shuffle , was neither funny or rather dramatic . while it tried so hard to make a point , because of this lack of definition ( comedy or drama ) , the clever themes and pointed remarks were lost . while i am a strong believer that there is too much racial profiling happening in hollywood , even today , i do not believe that townsend 's directorial debut did much</th>\n",
       "    <th>neg</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 i am curious ( yellow ) ( a film , in near xxunk rhyme , is said right at the start to be available in two versions , yellow and blue ) was one of those big art - house hits that first was a major sensation in sweden then a big scandal / cause - xxunk in the united states when the one print was held by customs and it went all the way to the supreme court . what 's potent in the picture today is not so much what might offend by way of what</th>\n",
       "    <th>pos</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxfld 1 i had lost faith in sooraj r. barjatya after the movie main prem ki xxunk hoon , then a year back now i saw promos for vivah which looked good . but i did n't want to waste my hard earned money watching it in cinema . when the film first came out on xxup dvd i rented it and watched and i loved the movie and took back my words for sooraj . i just finished watching it yesterday again and this time i thought i have to review this movie . sooraj r. xxunk it right</th>\n",
       "    <th>pos</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas = TextClasDataBunch.load(path, 'tmp_clas', bs=50)\n",
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a model to classify those reviews and load the encoder we saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, drop_mult=0.5)\n",
    "learn.load_encoder('fine_tuned_enc')\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 02:40\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      0.281750    0.209883    0.917120  (02:40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifierLearner(data=<fastai.text.data.TextClasDataBunch object at 0x7efb33525dd8>, model=SequentialRNN(\n",
       "  (0): MultiBatchRNNCore(\n",
       "    (encoder): Embedding(60002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function cross_entropy at 0x7efb3e3e1400>, metrics=[<function accuracy at 0x7efb381c1950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/.fastai/data/imdb'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[RNNTrainer(learn=TextClassifierLearner(data=<fastai.text.data.TextClasDataBunch object at 0x7efb33525dd8>, model=SequentialRNN(\n",
       "  (0): MultiBatchRNNCore(\n",
       "    (encoder): Embedding(60002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function cross_entropy at 0x7efb3e3e1400>, metrics=[<function accuracy at 0x7efb381c1950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/.fastai/data/imdb'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60002, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60002, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")]), bptt=70, alpha=2.0, beta=1.0, adjust=False)], layer_groups=[Sequential(\n",
       "  (0): Embedding(60002, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60002, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 02:57\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      0.234253    0.186893    0.929160  (02:57)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifierLearner(data=<fastai.text.data.TextClasDataBunch object at 0x7efb33525dd8>, model=SequentialRNN(\n",
       "  (0): MultiBatchRNNCore(\n",
       "    (encoder): Embedding(60002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function cross_entropy at 0x7efb3e3e1400>, metrics=[<function accuracy at 0x7efb381c1950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/.fastai/data/imdb'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[RNNTrainer(learn=TextClassifierLearner(data=<fastai.text.data.TextClasDataBunch object at 0x7efb33525dd8>, model=SequentialRNN(\n",
       "  (0): MultiBatchRNNCore(\n",
       "    (encoder): Embedding(60002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function cross_entropy at 0x7efb3e3e1400>, metrics=[<function accuracy at 0x7efb381c1950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/.fastai/data/imdb'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60002, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60002, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")]), bptt=70, alpha=2.0, beta=1.0, adjust=False)], layer_groups=[Sequential(\n",
       "  (0): Embedding(60002, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60002, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 04:02\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      0.218634    0.157757    0.941840  (04:02)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifierLearner(data=<fastai.text.data.TextClasDataBunch object at 0x7efb33525dd8>, model=SequentialRNN(\n",
       "  (0): MultiBatchRNNCore(\n",
       "    (encoder): Embedding(60002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function cross_entropy at 0x7efb3e3e1400>, metrics=[<function accuracy at 0x7efb381c1950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/.fastai/data/imdb'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[RNNTrainer(learn=TextClassifierLearner(data=<fastai.text.data.TextClasDataBunch object at 0x7efb33525dd8>, model=SequentialRNN(\n",
       "  (0): MultiBatchRNNCore(\n",
       "    (encoder): Embedding(60002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=<function cross_entropy at 0x7efb3e3e1400>, metrics=[<function accuracy at 0x7efb381c1950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/.fastai/data/imdb'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60002, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60002, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")]), bptt=70, alpha=2.0, beta=1.0, adjust=False)], layer_groups=[Sequential(\n",
       "  (0): Embedding(60002, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60002, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 09:52\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      0.180064    0.151520    0.944040  (04:55)\n",
      "2      0.142421    0.155700    0.944120  (04:56)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
